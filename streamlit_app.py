"""
Enhanced RAG Application with Google Gemini API
A powerful document Q&A system with web search capabilities
"""
import streamlit as st
import os
from typing import List, Dict
import time

# Import custom utilities
from utils.llm_client import GeminiClient
from utils.document_processor import DocumentProcessor
from utils.vector_store import VectorStore
from utils.web_search import WebSearcher

# Configure page
st.set_page_config(
    page_title="üß† Enhanced RAG Assistant",
    page_icon="üß†",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better UI
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .stButton > button {
        width: 100%;
        margin-top: 1rem;
    }
    .success-message {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        border-radius: 0.375rem;
        padding: 1rem;
        margin: 1rem 0;
    }
    .info-box {
        background-color: #e3f2fd;
        border-left: 4px solid #2196f3;
        padding: 1rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

def get_api_key() -> str:
    """Get Google API key from secrets or environment"""
    try:
        # Try Streamlit secrets first (for deployment)
        return st.secrets["GOOGLE_API_KEY"]
    except:
        # Fallback to environment variable
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            st.error("üîë Google API key not found! Please add GOOGLE_API_KEY to your Streamlit secrets or environment variables.")
            st.info("Get your free API key at: https://makersuite.google.com/app/apikey")
            st.stop()
        return api_key

def initialize_session_state():
    """Initialize session state variables"""
    # Core components
    if "vector_store" not in st.session_state:
        st.session_state.vector_store = VectorStore()
    
    # Processing status
    if "documents_processed" not in st.session_state:
        st.session_state.documents_processed = False
    if "processing_status" not in st.session_state:
        st.session_state.processing_status = ""
    if "last_query" not in st.session_state:
        st.session_state.last_query = ""
    if "last_response" not in st.session_state:
        st.session_state.last_response = ""
    
    # Chat and history
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "query_count" not in st.session_state:
        st.session_state.query_count = 0
    
    # Document stats
    if "document_stats" not in st.session_state:
        st.session_state.document_stats = []

def main():
    """Main application function"""
    
    # Initialize session state
    initialize_session_state()
    
    # Header
    st.markdown('<h1 class="main-header">üß† Enhanced RAG Assistant</h1>', unsafe_allow_html=True)
    st.markdown("### üìö Upload documents, ask questions, and get AI-powered answers with web search enhancement!")
    
    # Get API key
    api_key = get_api_key()
    
    # Initialize services
    web_searcher = WebSearcher(st.secrets.get("SERP_API_KEY", None) if "SERP_API_KEY" in st.secrets else None)
    
    # Sidebar configuration
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration")
        
        # Model selection
        available_models = GeminiClient.get_available_models()
        selected_model = st.selectbox(
            "ü§ñ Select Model",
            options=list(available_models.keys()),
            format_func=lambda x: f"{x} - {available_models[x]}",
            index=0
        )
        
        # Temperature control
        temperature = st.slider(
            "üå°Ô∏è Response Creativity (Temperature)",
            min_value=0.0,
            max_value=1.0,
            value=0.7,
            step=0.1,
            help="Lower values = more focused, Higher values = more creative"
        )
        
        # Perspective and audience settings
        st.subheader("üé≠ Response Style")
        perspective = st.selectbox(
            "üë§ Perspective",
            ["scientist", "engineer", "teacher", "policy expert", "AI researcher", "environmentalist", "physician", "consultant", "analyst"],
            index=0
        )
        
        audience = st.selectbox(
            "üéØ Target Audience", 
            ["students", "teachers", "laypeople", "policy makers", "researchers", "community members", "professionals", "executives"],
            index=0
        )
        
        # Search settings
        st.subheader("üîç Search Settings")
        enable_web_search = st.checkbox("Enable Web Search", value=True, help="Enhance answers with real-time web information")
        num_doc_results = st.slider("Document Results", 3, 10, 5)
        num_web_results = st.slider("Web Results", 1, 5, 2)
        
        # Vector store status
        st.subheader("üìä Status")
        doc_count = st.session_state.vector_store.get_collection_count()
        col_a, col_b = st.columns(2)
        col_a.metric("Documents", doc_count)
        col_b.metric("Queries", st.session_state.query_count)
        
        # Status indicators
        if st.session_state.documents_processed:
            st.success("‚úÖ Documents Ready")
        else:
            st.info("üìÑ Upload documents to begin")
            
        if st.session_state.last_response:
            st.success("üí¨ Ready for questions")
        
        # Quick actions
        st.subheader("‚ö° Quick Actions")
        
        if st.button("üóëÔ∏è Clear All Data"):
            # Clear all session state
            st.session_state.vector_store.clear_collection()
            st.session_state.documents_processed = False
            st.session_state.processing_status = ""
            st.session_state.last_query = ""
            st.session_state.last_response = ""
            st.session_state.chat_history = []
            st.session_state.query_count = 0
            st.session_state.document_stats = []
            st.success("üßπ All data cleared!")
            st.rerun()
    
    # Initialize LLM client
    llm_client = GeminiClient(api_key, selected_model)
    
    # Main content area - improved layout inspired by bio-app
    col1, col2 = st.columns([1, 2])  # Controls on left, results on right
    
    with col1:
        st.subheader("üìÑ Document Upload")
        
        uploaded_files = st.file_uploader(
            "Upload PDF documents",
            type=['pdf'],
            accept_multiple_files=True,
            help="Upload up to 10 PDF files (max 200MB each)"
        )
        
        if uploaded_files and st.button("üöÄ Process Documents"):
            if len(uploaded_files) > 10:
                st.warning("‚ö†Ô∏è Please upload maximum 10 files")
            else:
                with st.spinner("Processing documents..."):
                    # Process PDFs
                    documents = DocumentProcessor.process_multiple_pdfs(uploaded_files)
                    
                    if documents:
                        # Create chunks for each document
                        all_chunks = []
                        for filename, text in documents:
                            chunks = DocumentProcessor.chunk_text(text)
                            all_chunks.append(chunks)
                            
                            # Show document stats
                            stats = DocumentProcessor.get_document_stats(text)
                            with st.expander(f"üìä {filename} Stats"):
                                col_a, col_b, col_c = st.columns(3)
                                col_a.metric("Words", stats['words'])
                                col_b.metric("Characters", stats['characters'])
                                col_c.metric("Est. Tokens", stats['estimated_tokens'])
                        
                        # Add to vector store
                        st.session_state.vector_store.add_documents(documents, all_chunks)
                        st.session_state.documents_processed = True
                        st.session_state.processing_status = f"‚úÖ Processed {len(documents)} documents with {sum(len(chunks) for chunks in all_chunks)} total chunks"
                        st.session_state.document_stats = [(filename, DocumentProcessor.get_document_stats(text)) for filename, text in documents]
                        
                        st.success(st.session_state.processing_status)
    
    with col2:
        # Show processing status if available
        if st.session_state.processing_status:
            st.info(st.session_state.processing_status)
        
        # Show document stats
        if st.session_state.document_stats:
            with st.expander("üìä Document Statistics", expanded=False):
                for filename, stats in st.session_state.document_stats:
                    st.markdown(f"**{filename}:**")
                    col_a, col_b, col_c, col_d = st.columns(4)
                    col_a.metric("Words", stats['words'])
                    col_b.metric("Characters", stats['characters'])
                    col_c.metric("Lines", stats['lines'])
                    col_d.metric("Est. Tokens", stats['estimated_tokens'])
        
        st.subheader("üí¨ Ask Questions")
        
        # Query input
        query = st.text_area(
            "What would you like to know?",
            height=80,
            placeholder="Ask anything about your uploaded documents...",
            value=st.session_state.last_query
        )
        
        if query and st.button("üîç Get Answer"):
            if not st.session_state.documents_processed and st.session_state.vector_store.get_collection_count() == 0:
                st.warning("‚ö†Ô∏è Please upload and process documents first!")
            else:
                with st.spinner("üß† Generating answer..."):
                    
                    # Search documents
                    doc_results = st.session_state.vector_store.search(query, num_doc_results)
                    
                    # Search web if enabled
                    web_results = []
                    if enable_web_search:
                        web_results = web_searcher.search_web(query, num_web_results)
                    
                    # Prepare context
                    context_parts = []
                    
                    # Add document context
                    if doc_results:
                        context_parts.append("=== DOCUMENT CONTEXT ===")
                        for i, result in enumerate(doc_results, 1):
                            source = result['metadata'].get('source', 'Unknown')
                            context_parts.append(f"Source {i} ({source}):")
                            context_parts.append(result['content'])
                            context_parts.append("")
                    
                    # Add web context
                    if web_results:
                        context_parts.append("=== WEB SEARCH CONTEXT ===")
                        for i, result in enumerate(web_results, 1):
                            context_parts.append(f"Web Result {i}: {result['title']}")
                            context_parts.append(result['snippet'])
                            context_parts.append("")
                    
                    context = "\n".join(context_parts)
                    
                    # Create prompt
                    prompt = f"""You are responding as a {perspective}, and your response must be tailored for an audience of {audience}.

### Instructions:
- Use tone, vocabulary, and examples suitable for {audience}
- Explain in the way a {perspective} would ‚Äî with clarity, depth, and reasoning appropriate to the role
- Avoid jargon unless necessary, and define any technical terms
- Use the following context as background, not as quoted text
- If the context doesn't contain relevant information, say so clearly
- Integrate web search results naturally when available

### Context:
{context}

### Question:
{query}

### Answer (as a {perspective} to {audience}):"""
                    
                    # Generate response
                    response = llm_client.generate_response(prompt, temperature)
                    
                    # Update session state
                    st.session_state.last_query = query
                    st.session_state.last_response = response
                    st.session_state.query_count += 1
                    
                    # Display results
                    st.markdown("### üéØ Answer")
                    st.text_area("Generated Response", response, height=400)
                    
                    # Show sources
                    if doc_results or web_results:
                        with st.expander("üìö Sources Used"):
                            if doc_results:
                                st.markdown("**Document Sources:**")
                                for result in doc_results:
                                    source = result['metadata'].get('source', 'Unknown')
                                    st.markdown(f"- {source}")
                            
                            if web_results:
                                st.markdown("**Web Sources:**")
                                for result in web_results:
                                    if result['url']:
                                        st.markdown(f"- [{result['title']}]({result['url']})")
                                    else:
                                        st.markdown(f"- {result['title']}")
                    
                    # Add to chat history
                    st.session_state.chat_history.append({
                        "query": query,
                        "response": response,
                        "timestamp": time.time()
                    })
        
        # Show last response if available (even when not actively querying)
        if st.session_state.last_response and not query:
            st.markdown("### üí≠ Last Response")
            st.text_area("Previous Answer", st.session_state.last_response, height=300)
            
            # Show query count
            if st.session_state.query_count > 0:
                st.caption(f"Total queries processed: {st.session_state.query_count}")
    
    # Chat History
    if st.session_state.chat_history:
        st.subheader("üí≠ Recent Conversations")
        for i, chat in enumerate(reversed(st.session_state.chat_history[-5:]), 1):
            with st.expander(f"Q{i}: {chat['query'][:100]}..."):
                st.markdown(f"**Question:** {chat['query']}")
                st.markdown(f"**Answer:** {chat['response']}")
    
    # Footer
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: #666;'>
        <p>üöÄ Built for the Hackathon | Powered by Google Gemini & Streamlit Community Cloud</p>
        <p>üí° Upload PDFs, ask questions, get intelligent answers with web search enhancement!</p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()