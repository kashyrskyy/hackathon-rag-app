"""
Web Search Functionality
Provides web search capabilities to enhance RAG with real-time information
"""
import requests
from typing import List, Dict, Optional
import streamlit as st
from bs4 import BeautifulSoup
import time

try:
    from duckduckgo_search import DDGS
    DDGS_AVAILABLE = True
except ImportError:
    DDGS_AVAILABLE = False
    # Don't show warning here - it shows on every page load
    # st.warning("âš ï¸ duckduckgo-search not installed. Web search will use fallback method.")

class WebSearcher:
    """Handles web search operations"""
    
    def __init__(self, serp_api_key: Optional[str] = None):
        """
        Initialize web searcher
        
        Args:
            serp_api_key: Optional SerpAPI key for enhanced search
        """
        self.serp_api_key = serp_api_key
        self.last_search_time = 0
        self.min_search_interval = 3  # Minimum seconds between searches
    
    def search_web(self, query: str, num_results: int = 3) -> List[Dict[str, str]]:
        """
        Search the web for information
        
        Args:
            query: Search query
            num_results: Number of results to return
            
        Returns:
            List of search results with title, snippet, and URL
        """
        # Rate limiting check
        current_time = time.time()
        time_since_last = current_time - self.last_search_time
        
        if time_since_last < self.min_search_interval:
            remaining_wait = self.min_search_interval - time_since_last
            st.info(f"â±ï¸ Web search rate limited. Waiting {remaining_wait:.1f} seconds...")
            time.sleep(remaining_wait)
        
        self.last_search_time = time.time()
        
        try:
            if self.serp_api_key:
                st.info("ðŸ” Using SerpAPI for web search...")
                return self._search_with_serpapi(query, num_results)
            elif DDGS_AVAILABLE:
                st.info("ðŸ” Searching the web with DuckDuckGo...")
                return self._search_with_ddgs_library(query, num_results)
            else:
                st.info("ðŸ” Using DuckDuckGo API for web search...")
                return self._search_with_duckduckgo_api(query, num_results)
        except Exception as e:
            st.warning(f"Web search failed: {str(e)[:100]}...")
            return self._create_fallback_result(query)
    
    def _search_with_serpapi(self, query: str, num_results: int) -> List[Dict[str, str]]:
        """Search using SerpAPI (requires API key)"""
        try:
            url = "https://serpapi.com/search"
            params = {
                "q": query,
                "api_key": self.serp_api_key,
                "engine": "google",
                "num": num_results
            }
            
            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            results = []
            
            for result in data.get("organic_results", [])[:num_results]:
                results.append({
                    "title": result.get("title", ""),
                    "snippet": result.get("snippet", ""),
                    "url": result.get("link", "")
                })
            
            return results
            
        except Exception as e:
            st.warning(f"SerpAPI search failed: {str(e)}")
            if DDGS_AVAILABLE:
                return self._search_with_ddgs_library(query, num_results)
            else:
                return self._search_with_duckduckgo_api(query, num_results)
    
    def _search_with_ddgs_library(self, query: str, num_results: int) -> List[Dict[str, str]]:
        """Enhanced search using duckduckgo-search library with rate limiting handling"""
        max_retries = 3
        base_delay = 3
        
        for attempt in range(max_retries):
            try:
                results = []
                # Progressive delay to avoid rate limiting
                delay = base_delay * (attempt + 1) if attempt > 0 else 2
                time.sleep(delay)
                
                # Use different search strategies
                search_params = [
                    {"safesearch": "moderate", "timelimit": None},
                    {"safesearch": "off", "region": "us-en"},
                    {"max_results": min(num_results, 3), "timelimit": "m"}  # Recent results only
                ][attempt % 3]
                
                with DDGS() as ddgs:
                    search_results = ddgs.text(query, max_results=num_results, **search_params)
                    for result in search_results:
                        title = result.get("title", "")
                        snippet = result.get("body", "") or result.get("content", "")
                        url = result.get("href", "")
                        
                        # Skip if this looks like a fallback result
                        if "AI Knowledge Response" not in title and snippet and len(snippet) > 50:
                            results.append({
                                "title": title,
                                "snippet": snippet,
                                "url": url
                            })
                
                if results:
                    return results
                    
            except Exception as e:
                error_msg = str(e).lower()
                if any(term in error_msg for term in ["ratelimit", "202", "429", "blocked", "forbidden"]):
                    if attempt == max_retries - 1:
                        # Final attempt failed - try alternative search
                        return self._search_with_alternative_method(query, num_results)
                    continue  # Try again with longer delay
                else:
                    # Non-rate-limit error - try fallback immediately
                    break
        
        # All attempts failed - try alternative search
        return self._search_with_alternative_method(query, num_results)
    
    def _search_with_duckduckgo_api(self, query: str, num_results: int) -> List[Dict[str, str]]:
        """Fallback search using DuckDuckGo (no API key required)"""
        try:
            # Use DuckDuckGo's instant answer API
            url = "https://api.duckduckgo.com/"
            params = {
                "q": query,
                "format": "json",
                "no_html": "1",
                "skip_disambig": "1"
            }
            
            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            results = []
            
            # Try to get abstract
            if data.get("Abstract"):
                results.append({
                    "title": data.get("Heading", query),
                    "snippet": data.get("Abstract"),
                    "url": data.get("AbstractURL", "")
                })
            
            # Try to get related topics
            for topic in data.get("RelatedTopics", [])[:num_results-len(results)]:
                if isinstance(topic, dict) and topic.get("Text"):
                    results.append({
                        "title": topic.get("Text", "")[:100] + "...",
                        "snippet": topic.get("Text", ""),
                        "url": topic.get("FirstURL", "")
                    })
            
            return results if results else self._create_fallback_result(query)
            
        except Exception as e:
            st.warning(f"Web search failed: {str(e)}")
            return self._create_fallback_result(query)
    
    def _search_with_alternative_method(self, query: str, num_results: int) -> List[Dict[str, str]]:
        """Alternative search method when DuckDuckGo fails"""
        # Try DuckDuckGo API first
        ddg_results = self._search_with_duckduckgo_api(query, num_results)
        if ddg_results and not any("AI Knowledge Response" in result.get("title", "") for result in ddg_results):
            return ddg_results
        
        # Try a simple web scraping approach as last resort
        return self._search_with_simple_scraping(query, num_results)
    
    def _search_with_simple_scraping(self, query: str, num_results: int) -> List[Dict[str, str]]:
        """Simple web scraping when all other methods fail"""
        try:
            # Use a different search approach - search for academic papers if the query seems academic
            if any(term in query.lower() for term in ['paper', 'research', 'study', 'findings', 'compare', 'domain']):
                return self._create_academic_context_result(query)
            
            # For general queries, try a basic web request
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            # Try to get some basic web content
            search_url = f"https://html.duckduckgo.com/html/?q={query.replace(' ', '+')}"
            response = requests.get(search_url, headers=headers, timeout=5)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                results = []
                
                # Look for result links
                for link in soup.find_all('a', class_='result__a')[:num_results]:
                    title = link.get_text(strip=True)
                    url = link.get('href', '')
                    
                    if title and len(title) > 10:
                        results.append({
                            "title": title,
                            "snippet": f"Search result for: {query}",
                            "url": url
                        })
                
                return results if results else []
            
        except Exception:
            pass
        
        # If all else fails, return empty list
        return []
    
    def _create_academic_context_result(self, query: str) -> List[Dict[str, str]]:
        """Create contextual academic search result"""
        return [{
            "title": f"Academic Research Context: {query[:100]}",
            "snippet": "For academic research comparisons, consider searching databases like PubMed, Google Scholar, or ResearchGate for recent publications in the same domain. Compare methodologies, sample sizes, findings, and conclusions across studies.",
            "url": "https://pubmed.ncbi.nlm.nih.gov/"
        }]
    
    def _create_fallback_result(self, query: str) -> List[Dict[str, str]]:
        """Create a fallback result when search fails"""
        return [{
            "title": f"AI Knowledge Response: {query}",
            "snippet": f"Web search is temporarily unavailable. The AI will respond using its training knowledge and any documents you've uploaded. For real-time information, please try again in a few moments.",
            "url": ""
        }]
    
    def extract_text_from_url(self, url: str, max_chars: int = 1000) -> str:
        """
        Extract text content from a URL
        
        Args:
            url: URL to extract text from
            max_chars: Maximum characters to extract
            
        Returns:
            Extracted text content
        """
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Remove script and style elements
            for script in soup(["script", "style"]):
                script.decompose()
            
            # Get text
            text = soup.get_text()
            
            # Clean up text
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            return text[:max_chars]
            
        except Exception as e:
            return f"Could not extract content from {url}: {str(e)}"